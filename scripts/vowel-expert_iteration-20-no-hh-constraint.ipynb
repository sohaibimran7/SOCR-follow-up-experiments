{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.expert_iteration import ExpertIteration, ExpertIterationConfig, Evaluator, Log\n",
    "from src.evaluators import InspectEvaluator\n",
    "from src.samplers import InspectSampler\n",
    "from src.finetuners import OpenAIFinetuner\n",
    "from src.inspect_helpers.tasks import boolq_dataset_vowel_expert_iter\n",
    "from inspect_ai.log import list_eval_logs, read_eval_log\n",
    "import asyncio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoawait asyncio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expert Iterations to run\n",
    "\n",
    "GPT-4o \\\n",
    "GPT-4o-mini \\\n",
    "GPT-4o declarative finetuned \\\n",
    "GPT-4o-mini declarative finetuned \n",
    "\n",
    "We will run GPT-4o and GPT-4o-mini expert iteraions simultaneously while preparing the declarative finetuned models. Expert iterations for the declarative finetuned models will be run thereafter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = \"vowel_expert_iter_no_hhh_constraint\"\n",
    "LOG_DIR = f\"logs/{EXPERIMENT_NAME}/\"\n",
    "# BASE_MODEL_SUFFIX = \"base\"\n",
    "DECLARATIVE_FT_SUFFIX = \"QnA_augmentation_cd_n\"\n",
    "# DECLARATIVE_FT_FILE = \"data/declarative_ft_chat_models/QnA_augmentation_cd_n.jsonl\"\n",
    "CHECKPOINTS_TO_EVALUATE = [0]\n",
    "MODELS = [\n",
    "    \"gpt-4o-mini-2024-07-18\", \n",
    "    # \"gpt-4o-2024-08-06\",\n",
    "]\n",
    "\n",
    "MAX_ITER = 20\n",
    "N_TO_SAMPLE_FROM = 1000\n",
    "N_TO_SAMPLE = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_expert_iteration(model, model_suffix):\n",
    "    base_model = model.split(\":\")[1] if \":\" in model else model\n",
    "\n",
    "    config = ExpertIterationConfig(\n",
    "        max_iter=MAX_ITER,\n",
    "        modelprovider=\"openai\",\n",
    "        model=model,\n",
    "        log_dir=f\"{LOG_DIR}/{base_model}/{model_suffix}\",\n",
    "        retries=1,\n",
    "        suffix=f\"{EXPERIMENT_NAME}_{model_suffix}\",\n",
    "    )\n",
    "    evaluator = InspectEvaluator(\n",
    "        tasks=boolq_dataset_vowel_expert_iter(hhh_constraint=False),\n",
    "        limit=N_TO_SAMPLE_FROM,\n",
    "        max_connections=100,\n",
    "        timeout=600,\n",
    "    )\n",
    "    sampler = InspectSampler(\n",
    "        rank_column=\"scores.src/pattern_scorer.value\",\n",
    "        n=N_TO_SAMPLE,\n",
    "        conditions=[(\"scores.src/hhh_scorer.value\", \"C\")],\n",
    "    )\n",
    "    finetuner = OpenAIFinetuner(n_epochs=1, learning_rate_multiplier=2)\n",
    "\n",
    "    return ExpertIteration(\n",
    "        config=config, evaluator=evaluator, sampler=sampler, finetuner=finetuner\n",
    "    )\n",
    "\n",
    "\n",
    "async def get_expert_iter_tasks(models, model_suffix=\"base\"):\n",
    "    expert_iters = [create_expert_iteration(model, model_suffix) for model in models]\n",
    "    tasks = [expert_iter.run() for expert_iter in expert_iters]\n",
    "    return tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create tasks for expert iterations for base models\n",
    "# base_expert_iter_tasks = await get_expert_iter_tasks(MODELS, BASE_MODEL_SUFFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare declarative finetuned models \n",
    "\n",
    "Comment out this code block if you already have the declarative finetuned models from previous experiments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.utils import read_jsonl_file\n",
    "\n",
    "\n",
    "# async def get_declarative_ft_tasks(\n",
    "#     models=MODELS,\n",
    "#     model_suffix=DECLARATIVE_FT_SUFFIX,\n",
    "# ):\n",
    "#     finetuning_tasks = []\n",
    "#     for model in models:\n",
    "#         finetuner = OpenAIFinetuner(n_epochs=1, learning_rate_multiplier=2)\n",
    "#         finetuning_task = finetuner.run(\n",
    "#             model=model,\n",
    "#             input_log=read_jsonl_file(DECLARATIVE_FT_FILE),\n",
    "#             log_dir=f\"{LOG_DIR}/{model}/{model_suffix}/{model_suffix}\",\n",
    "#             suffix=model_suffix,\n",
    "#         )\n",
    "#         finetuning_tasks.append(finetuning_task)\n",
    "#     return finetuning_tasks\n",
    "\n",
    "\n",
    "# # Create tasks for declarative finetuning\n",
    "# declarative_ft_tasks = await get_declarative_ft_tasks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run expert iteraions and prepare the declarative finetuned models simultaneously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the declarative finetuned models from the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ft:gpt-4o-2024-08-06:personal:qna-augmentation-cd-n:A2dyGPjh:ckpt-step-900',\n",
       " 'ft:gpt-4o-mini-2024-07-18:personal:qna-augmentation-cd-n:9z7eelg2:ckpt-step-900']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from src.utils import get_finetunes, get_checkpoint_models\n",
    "\n",
    "CHECKPOINTS_TO_EVALUATE = [0]\n",
    "\n",
    "# declarative_ft_model_names = [\n",
    "#     job.fine_tuned_model\n",
    "#     for job in get_finetunes(OpenAI(), MODELS, DECLARATIVE_FT_SUFFIX)\n",
    "# ]\n",
    "\n",
    "declarative_ft_model_names = []\n",
    "for job in get_finetunes(OpenAI(), MODELS, DECLARATIVE_FT_SUFFIX):\n",
    "    declarative_ft_model_names.extend(\n",
    "        get_checkpoint_models(OpenAI(), job, CHECKPOINTS_TO_EVALUATE)\n",
    "    )\n",
    "\n",
    "declarative_ft_model_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the expert iterations for the declarative finetuned models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "declarative_ft_expert_iter_tasks = await get_expert_iter_tasks(\n",
    "    declarative_ft_model_names, DECLARATIVE_FT_SUFFIX\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/Users/work/SOCR-follow-up-experiments/src/evaluators.py:71: UserWarning: Concurrent eval_async call detected. \n",
       "Retrying...\n",
       "  warnings.warn(\"Concurrent eval_async call detected. Retrying...\")\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/Users/work/SOCR-follow-up-experiments/src/evaluators.py:71: UserWarning: Concurrent eval_async call detected. \n",
       "Retrying...\n",
       "  warnings.warn(\"Concurrent eval_async call detected. Retrying...\")\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7ea0ad904854291a41613f763722e88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfea2e25d2444f37aee40442d86e5d17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since boolq couldn't be found on the Hugging Face Hub\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[10/11/24 18:57:10] </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Using the latest cached version of the dataset since boolq couldn't be    <a href=\"file:///opt/anaconda3/envs/llm-awareness/lib/python3.12/site-packages/datasets/load.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">load.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/anaconda3/envs/llm-awareness/lib/python3.12/site-packages/datasets/load.py#1645\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1645</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         found on the Hugging Face Hub                                             <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[10/11/24 18:57:10]\u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Using the latest cached version of the dataset since boolq couldn't be    \u001b]8;id=399880;file:///opt/anaconda3/envs/llm-awareness/lib/python3.12/site-packages/datasets/load.py\u001b\\\u001b[2mload.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=214572;file:///opt/anaconda3/envs/llm-awareness/lib/python3.12/site-packages/datasets/load.py#1645\u001b\\\u001b[2m1645\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         found on the Hugging Face Hub                                             \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found the latest cached dataset configuration 'default' at /Users/work/.cache/huggingface/datasets/boolq/default/0.0.0/35b264d03638db9f4ce671b711558bf7ff0f80d5 (last modified on Fri Oct 11 18:52:02 2024).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Found the latest cached dataset configuration 'default' at                 <a href=\"file:///opt/anaconda3/envs/llm-awareness/lib/python3.12/site-packages/datasets/packaged_modules/cache/cache.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">cache.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/anaconda3/envs/llm-awareness/lib/python3.12/site-packages/datasets/packaged_modules/cache/cache.py#95\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">95</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         /Users/work/.cache/huggingface/datasets/boolq/default/0.0.0/35b264d03638db <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">           </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         9f4ce671b711558bf7ff0f80d5 (last modified on Fri Oct 11 18:52:02 2024).    <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">           </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Found the latest cached dataset configuration 'default' at                 \u001b]8;id=356291;file:///opt/anaconda3/envs/llm-awareness/lib/python3.12/site-packages/datasets/packaged_modules/cache/cache.py\u001b\\\u001b[2mcache.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=495923;file:///opt/anaconda3/envs/llm-awareness/lib/python3.12/site-packages/datasets/packaged_modules/cache/cache.py#95\u001b\\\u001b[2m95\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         /Users/work/.cache/huggingface/datasets/boolq/default/0.0.0/35b264d03638db \u001b[2m           \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         9f4ce671b711558bf7ff0f80d5 (last modified on Fri Oct 11 18:52:02 2024).    \u001b[2m           \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ade30795fbd943618e80bbdbf549f19f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b010c51fd484d3bb5572ad403cc8283",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b0b7ff31f4c4f42a4fcf134530d1794",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfe551dc6caf48f787334c3e85c2d3ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "221ff9621c894cd187791286b4704f65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0322079534c849389a3ceccdf950935a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75c3eb7bf217482cba493760e019653e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97783884cb2e4830a281dc0d3d4ac0c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d2fe4a5103c49c1a2e9f9acf7459da3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bccc3976ed8498f8018446726100fcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78f3898abb4249c3ae5f4f9d17c0f65e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/work/SOCR-follow-up-experiments/src/expert_iteration.py:153: UserWarning: Error Error code: 429 - {'error': {'message': \"This fine-tune request has been rate-limited. Your organization has reached the maximum of 8 fine-tuning requests per day for the model 'gpt-4o-2024-08-06'.\", 'type': 'invalid_request_error', 'param': None, 'code': 'daily_rate_limit_exceeded'}} encountered while running stage finetuning of iteration 6. Retrying 1 more times\n",
      "  warnings.warn(f\"Error {e} encountered while running stage {self.current_stage} of iteration {self.current_iter}. Retrying {remaining_retries} more times\")\n",
      "/Users/work/SOCR-follow-up-experiments/src/finetuners.py:68: UserWarning: Retrying fine-tuning...\n",
      "  warnings.warn(\"Retrying fine-tuning...\")\n",
      "/Users/work/SOCR-follow-up-experiments/src/expert_iteration.py:105: UserWarning: Iteration 6 failed: Error code: 429 - {'error': {'message': \"This fine-tune request has been rate-limited. Your organization has reached the maximum of 8 fine-tuning requests per day for the model 'gpt-4o-2024-08-06'.\", 'type': 'invalid_request_error', 'param': None, 'code': 'daily_rate_limit_exceeded'}}\n",
      "  warnings.warn(f\"Iteration {i} failed: {str(e)}\")\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': \"This fine-tune request has been rate-limited. Your organization has reached the maximum of 8 fine-tuning requests per day for the model 'gpt-4o-2024-08-06'.\", 'type': 'invalid_request_error', 'param': None, 'code': 'daily_rate_limit_exceeded'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m      8\u001b[0m     tasks_to_await\u001b[38;5;241m.\u001b[39mextend(declarative_ft_expert_iter_tasks)\n\u001b[0;32m----> 9\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39mtasks_to_await)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llm-awareness/lib/python3.12/asyncio/tasks.py:385\u001b[0m, in \u001b[0;36mTask.__wakeup\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__wakeup\u001b[39m(\u001b[38;5;28mself\u001b[39m, future):\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 385\u001b[0m         \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    387\u001b[0m         \u001b[38;5;66;03m# This may also be a cancellation.\u001b[39;00m\n\u001b[1;32m    388\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__step(exc)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llm-awareness/lib/python3.12/asyncio/tasks.py:314\u001b[0m, in \u001b[0;36mTask.__step_run_and_handle_result\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    312\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[1;32m    313\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[0;32m--> 314\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    316\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
      "File \u001b[0;32m~/SOCR-follow-up-experiments/src/expert_iteration.py:103\u001b[0m, in \u001b[0;36mExpertIteration.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_iter \u001b[38;5;241m=\u001b[39m i\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 103\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_iteration()\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    105\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIteration \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m failed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/SOCR-follow-up-experiments/src/expert_iteration.py:133\u001b[0m, in \u001b[0;36mExpertIteration._run_iteration\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(retries, RetryConfig):\n\u001b[1;32m    131\u001b[0m     retries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(retries, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_stage)\n\u001b[0;32m--> 133\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_stage(stage_obj, remaining_retries\u001b[38;5;241m=\u001b[39mretries)\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_retry \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(stage_obj, Finetuner):\n",
      "File \u001b[0;32m~/SOCR-follow-up-experiments/src/expert_iteration.py:154\u001b[0m, in \u001b[0;36mExpertIteration._try_stage\u001b[0;34m(self, stage_obj, remaining_retries)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_retry \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    153\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m encountered while running stage \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_stage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of iteration \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_iter\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Retrying \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mremaining_retries\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m more times\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_stage(stage_obj, remaining_retries \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_stage_failure(\u001b[38;5;28mstr\u001b[39m(e))\n",
      "File \u001b[0;32m~/SOCR-follow-up-experiments/src/expert_iteration.py:146\u001b[0m, in \u001b[0;36mExpertIteration._try_stage\u001b[0;34m(self, stage_obj, remaining_retries)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_stage\u001b[39m(\u001b[38;5;28mself\u001b[39m, stage_obj: AbstractStage, remaining_retries: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Log, Tuple[Log, \u001b[38;5;28mstr\u001b[39m]]:\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 146\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_stage(stage_obj)\n\u001b[1;32m    147\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_attempt_success()\n\u001b[1;32m    148\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/SOCR-follow-up-experiments/src/expert_iteration.py:168\u001b[0m, in \u001b[0;36mExpertIteration._run_stage\u001b[0;34m(self, stage_obj)\u001b[0m\n\u001b[1;32m    160\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodelprovider\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmodelprovider,\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msuffix\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_suffix()\n\u001b[1;32m    166\u001b[0m }\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_retry:\n\u001b[0;32m--> 168\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m stage_obj\u001b[38;5;241m.\u001b[39mretry(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m stage_obj\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/SOCR-follow-up-experiments/src/finetuners.py:69\u001b[0m, in \u001b[0;36mOpenAIFinetuner.retry\u001b[0;34m(self, model, input_log, log_dir, suffix, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mretry\u001b[39m(\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28mself\u001b[39m, model: \u001b[38;5;28mstr\u001b[39m, input_log: Any, log_dir: \u001b[38;5;28mstr\u001b[39m, suffix: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m     67\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Any, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m     68\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrying fine-tuning...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 69\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun(model, input_log, log_dir, suffix, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/SOCR-follow-up-experiments/src/finetuners.py:45\u001b[0m, in \u001b[0;36mOpenAIFinetuner.run\u001b[0;34m(self, model, input_log, log_dir, suffix, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m training_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_training_data(input_log)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Create a fine-tuning job\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mfine_tuning\u001b[38;5;241m.\u001b[39mjobs\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m     46\u001b[0m     training_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_upload_training_file(training_data, log_dir),\n\u001b[1;32m     47\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     48\u001b[0m     suffix\u001b[38;5;241m=\u001b[39msuffix,\n\u001b[1;32m     49\u001b[0m     hyperparameters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhyperparameters\n\u001b[1;32m     50\u001b[0m )\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Wait for the fine-tuning job to complete\u001b[39;00m\n\u001b[1;32m     53\u001b[0m job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_job_completion(job\u001b[38;5;241m.\u001b[39mid)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llm-awareness/lib/python3.12/site-packages/openai/resources/fine_tuning/jobs/jobs.py:435\u001b[0m, in \u001b[0;36mAsyncJobs.create\u001b[0;34m(self, model, training_file, hyperparameters, integrations, seed, suffix, validation_file, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    356\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    369\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    370\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m FineTuningJob:\n\u001b[1;32m    371\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;124;03m    Creates a fine-tuning job which begins the process of creating a new model from\u001b[39;00m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;124;03m    a given dataset.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;124;03m      timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 435\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[1;32m    436\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/fine_tuning/jobs\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    437\u001b[0m         body\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[1;32m    438\u001b[0m             {\n\u001b[1;32m    439\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[1;32m    440\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_file\u001b[39m\u001b[38;5;124m\"\u001b[39m: training_file,\n\u001b[1;32m    441\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhyperparameters\u001b[39m\u001b[38;5;124m\"\u001b[39m: hyperparameters,\n\u001b[1;32m    442\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mintegrations\u001b[39m\u001b[38;5;124m\"\u001b[39m: integrations,\n\u001b[1;32m    443\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[1;32m    444\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msuffix\u001b[39m\u001b[38;5;124m\"\u001b[39m: suffix,\n\u001b[1;32m    445\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_file\u001b[39m\u001b[38;5;124m\"\u001b[39m: validation_file,\n\u001b[1;32m    446\u001b[0m             },\n\u001b[1;32m    447\u001b[0m             job_create_params\u001b[38;5;241m.\u001b[39mJobCreateParams,\n\u001b[1;32m    448\u001b[0m         ),\n\u001b[1;32m    449\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[1;32m    450\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m    451\u001b[0m         ),\n\u001b[1;32m    452\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mFineTuningJob,\n\u001b[1;32m    453\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llm-awareness/lib/python3.12/site-packages/openai/_base_client.py:1831\u001b[0m, in \u001b[0;36mAsyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1817\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1818\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1819\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1826\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_AsyncStreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1827\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _AsyncStreamT:\n\u001b[1;32m   1828\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1829\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1830\u001b[0m     )\n\u001b[0;32m-> 1831\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llm-awareness/lib/python3.12/site-packages/openai/_base_client.py:1525\u001b[0m, in \u001b[0;36mAsyncAPIClient.request\u001b[0;34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1525\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m   1526\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1527\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   1528\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1529\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1530\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1531\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llm-awareness/lib/python3.12/site-packages/openai/_base_client.py:1611\u001b[0m, in \u001b[0;36mAsyncAPIClient._request\u001b[0;34m(self, cast_to, options, stream, stream_cls, retries_taken)\u001b[0m\n\u001b[1;32m   1609\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m   1610\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39maclose()\n\u001b[0;32m-> 1611\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[1;32m   1612\u001b[0m         input_options,\n\u001b[1;32m   1613\u001b[0m         cast_to,\n\u001b[1;32m   1614\u001b[0m         retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1615\u001b[0m         response_headers\u001b[38;5;241m=\u001b[39merr\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m   1616\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1617\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1618\u001b[0m     )\n\u001b[1;32m   1620\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1621\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llm-awareness/lib/python3.12/site-packages/openai/_base_client.py:1658\u001b[0m, in \u001b[0;36mAsyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1654\u001b[0m log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrying request to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m, options\u001b[38;5;241m.\u001b[39murl, timeout)\n\u001b[1;32m   1656\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m anyio\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1658\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m   1659\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   1660\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1661\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1662\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1663\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1664\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llm-awareness/lib/python3.12/site-packages/openai/_base_client.py:1611\u001b[0m, in \u001b[0;36mAsyncAPIClient._request\u001b[0;34m(self, cast_to, options, stream, stream_cls, retries_taken)\u001b[0m\n\u001b[1;32m   1609\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m   1610\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39maclose()\n\u001b[0;32m-> 1611\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[1;32m   1612\u001b[0m         input_options,\n\u001b[1;32m   1613\u001b[0m         cast_to,\n\u001b[1;32m   1614\u001b[0m         retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1615\u001b[0m         response_headers\u001b[38;5;241m=\u001b[39merr\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m   1616\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1617\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1618\u001b[0m     )\n\u001b[1;32m   1620\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1621\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llm-awareness/lib/python3.12/site-packages/openai/_base_client.py:1658\u001b[0m, in \u001b[0;36mAsyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1654\u001b[0m log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrying request to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m, options\u001b[38;5;241m.\u001b[39murl, timeout)\n\u001b[1;32m   1656\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m anyio\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1658\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m   1659\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   1660\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1661\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1662\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1663\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1664\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llm-awareness/lib/python3.12/site-packages/openai/_base_client.py:1626\u001b[0m, in \u001b[0;36mAsyncAPIClient._request\u001b[0;34m(self, cast_to, options, stream, stream_cls, retries_taken)\u001b[0m\n\u001b[1;32m   1623\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39maread()\n\u001b[1;32m   1625\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1626\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1629\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1630\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1634\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1635\u001b[0m )\n",
      "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': \"This fine-tune request has been rate-limited. Your organization has reached the maximum of 8 fine-tuning requests per day for the model 'gpt-4o-2024-08-06'.\", 'type': 'invalid_request_error', 'param': None, 'code': 'daily_rate_limit_exceeded'}}"
     ]
    }
   ],
   "source": [
    "# If declarative_ft_task is not defined (cell above is commented out), it won't be included\n",
    "tasks_to_await = base_expert_iter_tasks if \"base_expert_iter_tasks\" in locals() else []\n",
    "if \"declarative_ft_tasks\" in locals():\n",
    "    tasks_to_await.extend(declarative_ft_tasks)\n",
    "    print(\"Simultaneously  finetuning base model on declarative data and running expert iterations for the base models\")\n",
    "    await asyncio.gather(*tasks_to_await)\n",
    "    print(\"Running expert iterations for the declarative finetuned models\")\n",
    "    await asyncio.gather(*declarative_ft_expert_iter_tasks)\n",
    "else:\n",
    "    tasks_to_await.extend(declarative_ft_expert_iter_tasks)\n",
    "    print(\"Simultaneously running expert iterations for the base models and the declarative finetuned models\")\n",
    "    await asyncio.gather(*tasks_to_await)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-awareness",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
